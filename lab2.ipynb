{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparacion de las librerias a utilizar en el laboratorio"
      ],
      "metadata": {
        "id": "j9WFnV0S_hlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NOmE8Bql_RXT"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importamos el data set desde la libreria de Keras"
      ],
      "metadata": {
        "id": "L4Ao34lx_pUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos MNIST\n",
        "(X_entreno, y_entreno), (X_prueba, y_prueba) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalizar los datos\n",
        "X_entreno = X_entreno / 255.0\n",
        "X_prueba = X_prueba / 255.0\n",
        "\n",
        "# Aplanar las imágenes\n",
        "X_entreno = X_entreno.reshape(-1, 28*28)\n",
        "X_prueba = X_prueba.reshape(-1, 28*28)\n",
        "\n",
        "# Crear el modelo\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_entreno, y_entreno, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluar el modelo\n",
        "test_loss, test_acc = model.evaluate(X_prueba, y_prueba)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbAcYcNgARmO",
        "outputId": "dd3b84a9-49b7-40a3-fd1e-f1e696dff43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8686 - loss: 0.4716 - val_accuracy: 0.9569 - val_loss: 0.1493\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9613 - loss: 0.1323 - val_accuracy: 0.9663 - val_loss: 0.1175\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9758 - loss: 0.0863 - val_accuracy: 0.9690 - val_loss: 0.1013\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9812 - loss: 0.0629 - val_accuracy: 0.9741 - val_loss: 0.0928\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9862 - loss: 0.0456 - val_accuracy: 0.9747 - val_loss: 0.0860\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0371 - val_accuracy: 0.9735 - val_loss: 0.0890\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9920 - loss: 0.0288 - val_accuracy: 0.9709 - val_loss: 0.1043\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0218 - val_accuracy: 0.9758 - val_loss: 0.0896\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9948 - loss: 0.0186 - val_accuracy: 0.9761 - val_loss: 0.0898\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9960 - loss: 0.0138 - val_accuracy: 0.9747 - val_loss: 0.1002\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.1097\n",
            "Test accuracy: 0.9733999967575073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 1 - Modificar el tamaño de la capa oculta del modelo."
      ],
      "metadata": {
        "id": "KeIO3bZEAUZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Lista de tamaños de capa oculta a probar\n",
        "hidden_layer_sizes = [50, 100, 200, 300, 500]\n",
        "\n",
        "# Tabla para documentar resultados\n",
        "results = []\n",
        "\n",
        "for size in hidden_layer_sizes:\n",
        "    # Crear el modelo con el tamaño de capa oculta actual\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(size, activation='relu', input_shape=(784,)),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Medir el tiempo de entrenamiento\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_entreno, y_entreno, epochs=10, validation_split=0.2, verbose=0)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Obtener la precisión de validación y el tiempo de entrenamiento\n",
        "    val_acc = history.history['val_accuracy'][-1]\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Guardar los resultados\n",
        "    results.append((size, val_acc, training_time))\n",
        "\n",
        "# Imprimir los resultados\n",
        "for size, val_acc, training_time in results:\n",
        "    print(f'Tamaño de capa oculta: {size}, Precisión de validación: {val_acc}, Tiempo de entrenamiento: {training_time:.2f} segundos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G71O-GLAT4V",
        "outputId": "7bb1741e-e099-4da0-a08e-5bd08bb6d75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de capa oculta: 50, Precisión de validación: 0.9692500233650208, Tiempo de entrenamiento: 40.55 segundos\n",
            "Tamaño de capa oculta: 100, Precisión de validación: 0.9742500185966492, Tiempo de entrenamiento: 50.66 segundos\n",
            "Tamaño de capa oculta: 200, Precisión de validación: 0.9764999747276306, Tiempo de entrenamiento: 74.61 segundos\n",
            "Tamaño de capa oculta: 300, Precisión de validación: 0.9770833253860474, Tiempo de entrenamiento: 89.31 segundos\n",
            "Tamaño de capa oculta: 500, Precisión de validación: 0.9775833487510681, Tiempo de entrenamiento: 94.64 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ¿Cómo cambia la precisión de validación del modelo?\n",
        "  - Entre las iteraciones de capas que utilice la precisión no aumenta significativamente, pero si queremos un modelo perfecto el gap de entre 200 y 300 de capa oculta puede ser muy beneficioso porque la variación de presición no es mucha pero si que es una precisión muy buena.\n",
        "- ¿Cuánto tiempo tarda el algoritmo en entrenar?\n",
        "  - Resultados de tiempo arriba\n"
      ],
      "metadata": {
        "id": "WGYmSQtLiM1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 2  - Modificación de la Profundidad de la Red"
      ],
      "metadata": {
        "id": "7e5w5njIWvkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo con una capa oculta adicional\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Capa oculta adicional\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Medir el tiempo de entrenamiento\n",
        "start_time = time.time()\n",
        "history = model.fit(X_entreno, y_entreno, epochs=10, validation_split=0.2, verbose=0)\n",
        "end_time = time.time()\n",
        "\n",
        "# Obtener la precisión de validación y el tiempo de entrenamiento\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f'Precisión de validación con capa adicional: {val_acc}, Tiempo de entrenamiento: {training_time:.2f} segundos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "datOtJnYW1Zv",
        "outputId": "b5cfe2fb-be86-4ef4-df07-b60a413d1169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión de validación con capa adicional: 0.9747499823570251, Tiempo de entrenamiento: 62.39 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compare la precisión de validación con el modelo original\n",
        "  - El aumento de presición no es demasiado ya que el original ya tenía una presición muy cercana, pero en cuanto a precisión neta si que mejora al modelo original.\n",
        "- Analice el impacto en el tiempo de ejecución\n",
        "  - Me quedaría con el modelo original, precisión ligeramente menor pero un tiempo más corto, obviamente en cuanto escalemos los modelos y la dimensión sea más grande me quedaría con este de profundidad mayor ya que el gap si que llegaría a aumentar demasiado.\n",
        "- Explique los cambios necesarios en el código para implementar esta modificación\n",
        "  - Se agrego una capa más como se muestra arriba y es de tipo relu también."
      ],
      "metadata": {
        "id": "f1Chbf0Ti_Nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 3 - Redes Profundas"
      ],
      "metadata": {
        "id": "1OjW7AUvW43s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de profundidades a probar\n",
        "depths = [2, 3, 4, 5]\n",
        "\n",
        "# Tabla para documentar resultados\n",
        "results_depth = []\n",
        "\n",
        "for depth in depths:\n",
        "    # Crear el modelo con la profundidad actual\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,))\n",
        "    ])\n",
        "\n",
        "    for _ in range(depth - 1):\n",
        "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Medir el tiempo de entrenamiento\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_entreno, y_entreno, epochs=10, validation_split=0.2, verbose=0)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Obtener la precisión de validación y el tiempo de entrenamiento\n",
        "    val_acc = history.history['val_accuracy'][-1]\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Guardar los resultados\n",
        "    results_depth.append((depth, val_acc, training_time))\n",
        "\n",
        "# Imprimir los resultados\n",
        "for depth, val_acc, training_time in results_depth:\n",
        "    print(f'Profundidad: {depth}, Precisión de validación: {val_acc}, Tiempo de entrenamiento: {training_time:.2f} segundos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzjfC7zoW86w",
        "outputId": "bb257ef3-891e-4e83-c179-c1306218da23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profundidad: 2, Precisión de validación: 0.9737499952316284, Tiempo de entrenamiento: 61.79 segundos\n",
            "Profundidad: 3, Precisión de validación: 0.9780833125114441, Tiempo de entrenamiento: 78.94 segundos\n",
            "Profundidad: 4, Precisión de validación: 0.9739166498184204, Tiempo de entrenamiento: 67.85 segundos\n",
            "Profundidad: 5, Precisión de validación: 0.9765833616256714, Tiempo de entrenamiento: 78.26 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Analice la relación entre profundidad y tiempo de ejecución\n",
        " - Creo que es bastante evidente la relación que entre más profundidad el tiempo también aumenta aunque en este caso hubo una anomalia en la de profundidad 4 tardando menos que la de profundidad 3.\n",
        "- Identifique posibles problemas de desvanecimiento del gradiente\n",
        "  Puede ser que al llegar a cierta \"anchura\" el modelo llegue a perder mejora y el desvanecimiento del gradiente ya no llegue a cambiar."
      ],
      "metadata": {
        "id": "LkULir8aj5O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 4 - Funciones de Activación I"
      ],
      "metadata": {
        "id": "MstIylftXAR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo con activación sigmoidal\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='sigmoid', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Medir el tiempo de entrenamiento\n",
        "start_time = time.time()\n",
        "history = model.fit(X_entreno, y_entreno, epochs=10, validation_split=0.2, verbose=0)\n",
        "end_time = time.time()\n",
        "\n",
        "# Obtener la precisión de validación y el tiempo de entrenamiento\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f'Precisión de validación con activación sigmoidal: {val_acc}, Tiempo de entrenamiento: {training_time:.2f} segundos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zMWmXdXXGaU",
        "outputId": "f75c8398-2baf-428d-d24d-26bacf364aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión de validación con activación sigmoidal: 0.9738333225250244, Tiempo de entrenamiento: 74.03 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 5 - Funciones de Activación II"
      ],
      "metadata": {
        "id": "DjXf2Of0XG-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo con ReLU en la primera capa y tanh en la segunda\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(128, activation='tanh'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Medir el tiempo de entrenamiento\n",
        "start_time = time.time()\n",
        "history = model.fit(X_entreno, y_entreno, epochs=10, validation_split=0.2, verbose=0)\n",
        "end_time = time.time()\n",
        "\n",
        "# Obtener la precisión de validación y el tiempo de entrenamiento\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f'Precisión de validación con ReLU y tanh: {val_acc}, Tiempo de entrenamiento: {training_time:.2f} segundos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6aNsiwZXNPW",
        "outputId": "d5ad4b87-053b-438f-a728-5396c2a2bd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión de validación con ReLU y tanh: 0.9775000214576721, Tiempo de entrenamiento: 73.07 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compare el rendimiento con las configuraciones anteriores\n",
        "  - Rendimiento muy parecido al de Depth mayor y minimamente superior al modelo original.\n",
        "- Explique las ventajas y desventajas de cada función de activación\n",
        " - Las ventajas creo que tendrían que ver con que una es lineal y la otra no, Tanh es una función no lineal más fuerte que la sigmoide, lo que puede ayudar a la red a aprender representaciones más complejas, pero igualemente es más intensiva que RElu por que requiere más calculos computacionales."
      ],
      "metadata": {
        "id": "yf44a01KpQV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 6 - Tamaño de Batch Grande"
      ],
      "metadata": {
        "id": "jE6mghRpXQB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo base\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Medir el tiempo de entrenamiento con batch size grande\n",
        "start_time = time.time()\n",
        "history = model.fit(X_entreno, y_entreno, epochs=10, validation_split=0.2, batch_size=10000, verbose=0)\n",
        "end_time = time.time()\n",
        "\n",
        "# Obtener la precisión de validación y el tiempo de entrenamiento\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f'Precisión de validación con batch size 10,000: {val_acc}, Tiempo de entrenamiento: {training_time:.2f} segundos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXcZWYkoXSXk",
        "outputId": "4fd4edec-c9d7-4b9c-f17b-05f718e53f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión de validación con batch size 10,000: 0.9099166393280029, Tiempo de entrenamiento: 8.55 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Documente el cambio en el tiempo de entrenamiento\n",
        "  - Pues el tiempo es menor porque el batch es mayor con lo que el tiempo de iteracion se reduce y con esto tambien el tiempo que tarda el modelo en entrenarse al igual que tambien puede ser significativo que los pesos se actualizan con menor frecuencia por la cantidad de iteraciones menores que hay.\n",
        "- Analice el impacto en la precisión del modelo\n",
        " - Como dije anteriormente menos iteraciones, menos calculos de pesos por lo que la presición es buena pero vemos como es que la cantidad de iteraciones reducidas si que afecta de buena manera la presición siendo casi 7% menos efectiva que cualquiera de los modelos anteriores, incluso el original.\n",
        "- Explique teóricamente por qué se observan estos cambios\n",
        " - Menos iteraciones, menos calculo de pesos y también es imporante saber que con esta dimensión de batch el modelo puede llegar a estar en un minimo local y no saberlo con exactitud."
      ],
      "metadata": {
        "id": "W58Be_grpSIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 7 - Descenso de Gradiente Estocástico (SGD)"
      ],
      "metadata": {
        "id": "Ubf-cp2IXWUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo base\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Medir el tiempo de entrenamiento con SGD puro\n",
        "start_time = time.time()\n",
        "history = model.fit(X_entreno, y_entreno, epochs=10, validation_split=0.2, batch_size=1, verbose=0)\n",
        "end_time = time.time()\n",
        "\n",
        "# Obtener la precisión de validación y el tiempo de entrenamiento\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f'Precisión de validación con SGD puro: {val_acc}, Tiempo de entrenamiento: {training_time:.2f} segundos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4P8Cq4bXeBM",
        "outputId": "580e0b5b-b577-48b2-8ffd-f33b319cfa37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión de validación con SGD puro: 0.9707499742507935, Tiempo de entrenamiento: 1361.04 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compare el tiempo de ejecución con configuraciones anteriores\n",
        "  - Precisión ligeremante menor a los modelos de depth, RElu  y original, pero mejor que la de batch 10, precisión relativamente buena, pero el tiempo de entrenamiento es una gran desventaja ya que llego a tardar hasta 6 veces más que el modelo más tardado anteriormente.\n",
        "- Analice la estabilidad y precisión del entrenamiento\n",
        "  - Las actualizaciones de pesos son más ruidosas y menos estables, pero pueden resultar en una precisión más alta en el conjunto de validación debido a una exploración más detallada del espacio de búsqueda. Quizás en este caso en especifico esta estabilidad no llega a compensar en cuanto a la presición, pero quizás en sets muchisimo más pesados el detalle de la busqueda si que pueda compensar con lo tardado que es.\n",
        "- Explique si los resultados son coherentes con la teoría\n",
        "  - El tiempo de ejecución debe ser más largo debido a las muchas más iteraciones y la menor eficiencia de la GPU. Esto es coherente con la teoría, fue el modelo que más tardo con muchisima diferencia."
      ],
      "metadata": {
        "id": "EDGyGhPlpVJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 8 - Tasa de Apredizaje Baja\n",
        "\n"
      ],
      "metadata": {
        "id": "e67r6mCZd1de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"lr_low\"\n",
        "low_lr = 0.0001\n",
        "model_low_lr = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_low_lr.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=low_lr),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "history_low_lr = model_low_lr.fit(X_entreno, y_entreno,\n",
        "                                  epochs=10,\n",
        "                                  validation_split=0.2,\n",
        "                                  verbose=0)\n",
        "end_time = time.time()\n",
        "\n",
        "val_acc_low_lr = history_low_lr.history['val_accuracy'][-1]\n",
        "training_time_low_lr = end_time - start_time\n",
        "experiments[name] = {'history': history_low_lr, 'model': model_low_lr, 'train_time': training_time_low_lr}\n",
        "\n",
        "print(f'Tasa de aprendizaje baja ({low_lr}): '\n",
        "      f'Precisión de validación: {val_acc_low_lr:.4f}, '\n",
        "      f'Tiempo de entrenamiento: {training_time_low_lr:.2f} s')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8t9Sdgfd_ch",
        "outputId": "77511489-522a-4712-b525-3306bc606ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasa de aprendizaje baja (0.0001): Precisión de validación: 0.9693, Tiempo de entrenamiento: 87.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 9 - Tasa de Apredizaje Alta\n",
        "\n"
      ],
      "metadata": {
        "id": "QkOxxnUAniIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"lr_high\"\n",
        "high_lr = 0.02\n",
        "model_high_lr = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model_high_lr.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=high_lr),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "history_high_lr = model_high_lr.fit(X_entreno, y_entreno,\n",
        "                                    epochs=10,\n",
        "                                    validation_split=0.2,\n",
        "                                    verbose=0)\n",
        "end_time = time.time()\n",
        "\n",
        "val_acc_high_lr = history_high_lr.history['val_accuracy'][-1]\n",
        "training_time_high_lr = end_time - start_time\n",
        "experiments[name] = {'history': history_high_lr, 'model': model_high_lr, 'train_time': training_time_high_lr}\n",
        "\n",
        "test_loss_high, test_acc_high = model_high_lr.evaluate(X_prueba, y_prueba, verbose=0)\n",
        "\n",
        "print(\n",
        "    f\"Tasa de aprendizaje alta ({high_lr}): \"\n",
        "    f\"Pérdida prueba: {test_loss_high:.4f}, \"\n",
        "    f\"Precisión prueba: {test_acc_high:.4f}, \"\n",
        "    f\"Val Acc: {history_high_lr.history['val_accuracy'][-1]:.4f}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwSlxf3OeHyz",
        "outputId": "87e2c132-8f7d-4fb4-9b43-8b9fa0b99ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasa de aprendizaje alta (0.02): Precisión de validación: 0.9497, Tiempo de entrenamiento: 92.72 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Análisis comparativo\n",
        "- Precisión de validación\n",
        "\n",
        "LR baja supera en ~2 puntos porcentuales a LR alta (0.9693 vs. 0.9497), mostrando que los pasos más pequeños permiten encontrar un mínimo más preciso.\n",
        "\n",
        "LR alta tiende a estabilizarse en un plateau inferior debido a saltos excesivos en el espacio de pesos.\n",
        "\n",
        "- Velocidad de entrenamiento\n",
        "\n",
        "Curiosamente, LR baja resultó algo más rápida (≈5 s menos). Esto puede deberse a diferencias en la estabilidad interna del optimizador: pasos más grandes requieren ocasionalmente correcciones adicionales (momentum, cálculos extra) que encarecen ligeramente cada batch.\n",
        "\n",
        "- Estabilidad vs. agresividad\n",
        "\n",
        "Con LR alta es habitual observar oscilaciones en la curva de pérdida y accuracy, riesgo de divergencia parcial.\n",
        "\n",
        "Con LR baja, la curva es monótona y suave, reduciendo riesgo de saltarse el valle del óptimo local."
      ],
      "metadata": {
        "id": "zq5eO5K7ocx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 10 - Optimización Avanzada\n",
        "\n"
      ],
      "metadata": {
        "id": "_WirY33TnlHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_rates = [0.2, 0.5]\n",
        "results_dropout = []\n",
        "\n",
        "for rate in drop_rates:\n",
        "    name = f\"dropout_{int(rate*100)}\"\n",
        "    model_do = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "        Dropout(rate),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        Dropout(rate),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model_do.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    start = time.time()\n",
        "    hist = model_do.fit(X_entreno, y_entreno,\n",
        "                        epochs=10,\n",
        "                        validation_split=0.2,\n",
        "                        verbose=0)\n",
        "    end = time.time()\n",
        "    test_loss_do, test_acc_do = model_do.evaluate(X_prueba, y_prueba, verbose=0)\n",
        "    print(f\"Dropout {rate} — Pérdida prueba: {test_loss_do:.4f}, Precisión prueba: {test_acc_do:.4f}, Val Acc: {hist.history['val_accuracy'][-1]:.4f}\")\n",
        "    experiments[name] = {'history': hist, 'model': model_do, 'train_time': end - start}\n",
        "\n",
        "    results_dropout.append((rate,\n",
        "                            hist.history['val_accuracy'][-1],\n",
        "                            end - start))\n",
        "\n",
        "for rate, acc, t in results_dropout:\n",
        "    print(f'Dropout {rate}: Val Acc={acc:.4f}, Tiempo={t:.2f}s')\n",
        "\n",
        "l2_coefs = [1e-3, 1e-4]\n",
        "results_l2 = []\n",
        "\n",
        "for coef in l2_coefs:\n",
        "    name = f\"l2_{coef}\"\n",
        "    model_l2 = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,),\n",
        "                              kernel_regularizer=regularizers.l2(coef)),\n",
        "        tf.keras.layers.Dense(128, activation='relu',\n",
        "                              kernel_regularizer=regularizers.l2(coef)),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model_l2.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    start = time.time()\n",
        "    hist = model_l2.fit(X_entreno, y_entreno,\n",
        "                        epochs=10,\n",
        "                        validation_split=0.2,\n",
        "                        verbose=0)\n",
        "    end = time.time()\n",
        "    test_loss_l2, test_acc_l2 = model_l2.evaluate(X_prueba, y_prueba, verbose=0)\n",
        "    print(f\"L2 coef {coef} — Pérdida prueba: {test_loss_l2:.4f}, Precisión prueba: {test_acc_l2:.4f}, Val Acc: {hist.history['val_accuracy'][-1]:.4f}\")\n",
        "    results_l2.append((coef,\n",
        "                       hist.history['val_accuracy'][-1],\n",
        "                       end - start))\n",
        "    experiments[name] = {'history': hist, 'model': model_l2, 'train_time': end - start}\n",
        "\n",
        "for coef, acc, t in results_l2:\n",
        "    print(f'L2 coef {coef}: Val Acc={acc:.4f}, Tiempo={t:.2f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkwlATZQeZud",
        "outputId": "2bf9840f-7824-40ce-fc75-8f3b04e5dede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout 0.2 — Pérdida prueba: 0.0714, Precisión prueba: 0.9797, Val Acc: 0.9788\n",
            "Dropout 0.5 — Pérdida prueba: 0.1057, Precisión prueba: 0.9698, Val Acc: 0.9687\n",
            "Dropout 0.2: Val Acc=0.9788, Tiempo=97.10s\n",
            "Dropout 0.5: Val Acc=0.9687, Tiempo=100.66s\n",
            "L2 coef 0.001 — Pérdida prueba: 0.1747, Precisión prueba: 0.9713, Val Acc: 0.9678\n",
            "L2 coef 0.0001 — Pérdida prueba: 0.1263, Precisión prueba: 0.9781, Val Acc: 0.9773\n",
            "L2 coef 0.001: Val Acc=0.9678, Tiempo=93.76s\n",
            "L2 coef 0.0001: Val Acc=0.9773, Tiempo=100.95s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moderada regularización (Dropout 0.2 o L2 con coeficiente 1 × 10⁻⁴) es la más eficaz:\n",
        "\n",
        "Reduce el sobreajuste manteniendo alta la precisión de prueba.\n",
        "\n",
        "Gap muy bajo (< 0.1 %) demuestra sólida generalización.\n",
        "\n",
        "Evitar tasas demasiado altas (Dropout 0.5 o L2 1 × 10⁻³), pues inducen underfitting y empeoran la pérdida.\n",
        "\n",
        "Entre las dos, Dropout 0.2 se lleva la delantera por su menor pérdida final y ligera ventaja en precisión."
      ],
      "metadata": {
        "id": "jGOdmup1n0Te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 11 - Visualización"
      ],
      "metadata": {
        "id": "7nBL-u-Tj6RU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMVTSXwVnGSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inciso 12 - Modelo Óptimo"
      ],
      "metadata": {
        "id": "fJHMFZzekJoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers, callbacks\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# 1. Carga y preprocesamiento\n",
        "(X_entreno, y_entreno), (X_prueba, y_prueba) = tf.keras.datasets.mnist.load_data()\n",
        "X_entreno = X_entreno.astype(\"float32\") / 255.0\n",
        "X_prueba  = X_prueba.astype(\"float32\")  / 255.0\n",
        "# Aplanar\n",
        "X_entreno = X_entreno.reshape(-1, 28*28)\n",
        "X_prueba  = X_prueba.reshape(-1, 28*28)\n",
        "\n",
        "# 2. Arquitectura\n",
        "model = Sequential([\n",
        "    Input(shape=(784,)),\n",
        "\n",
        "    # Capa oculta 1\n",
        "    Dense(128, activation='relu',\n",
        "          kernel_regularizer=regularizers.l2(1e-4)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Capa oculta 2\n",
        "    Dense(128, activation='relu',\n",
        "          kernel_regularizer=regularizers.l2(1e-4)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Capa de salida\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. Compilación con LR bajo y scheduler\n",
        "initial_lr = 1e-4\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Reduce LR si la validación se estanca\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Early stopping para evitar overfitting\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4. Entrenamiento\n",
        "history = model.fit(\n",
        "    X_entreno, y_entreno,\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[reduce_lr, early_stop],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# 5. Evaluación final\n",
        "test_loss, test_acc = model.evaluate(X_prueba, y_prueba, verbose=0)\n",
        "print(f\"Pérdida de prueba: {test_loss:.4f}, Precisión de prueba: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyjXbXORpDlP",
        "outputId": "cd16e499-178a-459d-c18d-436b1fc8fd5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/30\n",
            "375/375 - 6s - 15ms/step - accuracy: 0.6827 - loss: 1.0549 - val_accuracy: 0.8895 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.8664 - loss: 0.4833 - val_accuracy: 0.9214 - val_loss: 0.3010 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "375/375 - 2s - 7ms/step - accuracy: 0.8988 - loss: 0.3729 - val_accuracy: 0.9363 - val_loss: 0.2531 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "375/375 - 4s - 11ms/step - accuracy: 0.9152 - loss: 0.3177 - val_accuracy: 0.9448 - val_loss: 0.2221 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "375/375 - 3s - 7ms/step - accuracy: 0.9261 - loss: 0.2808 - val_accuracy: 0.9507 - val_loss: 0.2013 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9348 - loss: 0.2508 - val_accuracy: 0.9542 - val_loss: 0.1869 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "375/375 - 3s - 7ms/step - accuracy: 0.9407 - loss: 0.2288 - val_accuracy: 0.9575 - val_loss: 0.1746 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9449 - loss: 0.2123 - val_accuracy: 0.9602 - val_loss: 0.1650 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "375/375 - 3s - 9ms/step - accuracy: 0.9492 - loss: 0.1964 - val_accuracy: 0.9623 - val_loss: 0.1576 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "375/375 - 4s - 11ms/step - accuracy: 0.9533 - loss: 0.1856 - val_accuracy: 0.9639 - val_loss: 0.1508 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9551 - loss: 0.1753 - val_accuracy: 0.9653 - val_loss: 0.1447 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9577 - loss: 0.1644 - val_accuracy: 0.9663 - val_loss: 0.1399 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "375/375 - 3s - 7ms/step - accuracy: 0.9606 - loss: 0.1561 - val_accuracy: 0.9675 - val_loss: 0.1356 - learning_rate: 1.0000e-04\n",
            "Epoch 14/30\n",
            "375/375 - 3s - 7ms/step - accuracy: 0.9626 - loss: 0.1506 - val_accuracy: 0.9683 - val_loss: 0.1309 - learning_rate: 1.0000e-04\n",
            "Epoch 15/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9642 - loss: 0.1421 - val_accuracy: 0.9704 - val_loss: 0.1295 - learning_rate: 1.0000e-04\n",
            "Epoch 16/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9660 - loss: 0.1369 - val_accuracy: 0.9707 - val_loss: 0.1244 - learning_rate: 1.0000e-04\n",
            "Epoch 17/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9684 - loss: 0.1296 - val_accuracy: 0.9705 - val_loss: 0.1222 - learning_rate: 1.0000e-04\n",
            "Epoch 18/30\n",
            "375/375 - 3s - 8ms/step - accuracy: 0.9699 - loss: 0.1238 - val_accuracy: 0.9715 - val_loss: 0.1194 - learning_rate: 1.0000e-04\n",
            "Epoch 19/30\n",
            "375/375 - 3s - 8ms/step - accuracy: 0.9705 - loss: 0.1204 - val_accuracy: 0.9724 - val_loss: 0.1184 - learning_rate: 1.0000e-04\n",
            "Epoch 20/30\n",
            "375/375 - 4s - 11ms/step - accuracy: 0.9718 - loss: 0.1155 - val_accuracy: 0.9728 - val_loss: 0.1160 - learning_rate: 1.0000e-04\n",
            "Epoch 21/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9740 - loss: 0.1106 - val_accuracy: 0.9742 - val_loss: 0.1142 - learning_rate: 1.0000e-04\n",
            "Epoch 22/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9740 - loss: 0.1072 - val_accuracy: 0.9738 - val_loss: 0.1135 - learning_rate: 1.0000e-04\n",
            "Epoch 23/30\n",
            "375/375 - 3s - 9ms/step - accuracy: 0.9754 - loss: 0.1032 - val_accuracy: 0.9759 - val_loss: 0.1110 - learning_rate: 1.0000e-04\n",
            "Epoch 24/30\n",
            "375/375 - 4s - 11ms/step - accuracy: 0.9759 - loss: 0.1011 - val_accuracy: 0.9754 - val_loss: 0.1089 - learning_rate: 1.0000e-04\n",
            "Epoch 25/30\n",
            "375/375 - 3s - 7ms/step - accuracy: 0.9771 - loss: 0.0985 - val_accuracy: 0.9747 - val_loss: 0.1088 - learning_rate: 1.0000e-04\n",
            "Epoch 26/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9783 - loss: 0.0936 - val_accuracy: 0.9755 - val_loss: 0.1084 - learning_rate: 1.0000e-04\n",
            "Epoch 27/30\n",
            "375/375 - 4s - 10ms/step - accuracy: 0.9787 - loss: 0.0917 - val_accuracy: 0.9762 - val_loss: 0.1079 - learning_rate: 1.0000e-04\n",
            "Epoch 28/30\n",
            "375/375 - 4s - 11ms/step - accuracy: 0.9792 - loss: 0.0899 - val_accuracy: 0.9762 - val_loss: 0.1051 - learning_rate: 1.0000e-04\n",
            "Epoch 29/30\n",
            "375/375 - 2s - 6ms/step - accuracy: 0.9803 - loss: 0.0876 - val_accuracy: 0.9758 - val_loss: 0.1064 - learning_rate: 1.0000e-04\n",
            "Epoch 30/30\n",
            "375/375 - 3s - 7ms/step - accuracy: 0.9817 - loss: 0.0828 - val_accuracy: 0.9760 - val_loss: 0.1045 - learning_rate: 1.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 30.\n",
            "Pérdida de prueba: 0.1021, Precisión de prueba: 0.9755\n"
          ]
        }
      ]
    }
  ]
}